{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Sentiment Classification (Inference)**"
      ],
      "metadata": {
        "id": "MEf7e2v8LxxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import Libraries**"
      ],
      "metadata": {
        "id": "qkSZ1AkaMQD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S92kAs60LmEx",
        "outputId": "5b7230c7-bae1-444a-9001-747ccb3e91c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#Import some required packages and libraries\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install wordcloud sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Pre-trained Models & Tokenizers**"
      ],
      "metadata": {
        "id": "HSyoE3rjMapR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pre-trained LSTM model\n",
        "lstm_model = load_model(\"lstm_sentiment_model.keras\")\n",
        "\n",
        "# Load Pre-trained SVM model\n",
        "with open(\"svm_model.pkl\", \"rb\") as f:\n",
        "    svm_model = pickle.load(f)\n",
        "\n",
        "# Load Pre-trained Logistic Regression model\n",
        "with open(\"logreg_model.pkl\", \"rb\") as f:\n",
        "    logreg_model = pickle.load(f)\n",
        "\n",
        "# Load TF-IDF Vectorizer\n",
        "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
        "    tfidf = pickle.load(f)\n",
        "\n",
        "# Load Tokenizer for LSTM\n",
        "with open(\"tokenizer_lstm.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSviJZvtMf0Q",
        "outputId": "549707d6-c4a2-4bdc-b843-654d9587c5d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Text Preprocessing**"
      ],
      "metadata": {
        "id": "P2kdadU9PTOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean text for sentiment analysis (Cleaning Text)\n",
        "def cleanText(text):\n",
        "    text = text.lower() #Convert text to lowercase\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text) #Remove mentions (@username)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text) #Remove hashtags (#hashtag)\n",
        "    text = re.sub(r\"http\\S+\", '', text) #Remove URLs (https://...)\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text, flags=re.UNICODE) #Remove emojis\n",
        "    text = re.sub(r'\\d+', '', text) #Remove numbers\n",
        "    text = text.replace('\\n', ' ') #Replace newlines with space\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) #Remove all punctuation\n",
        "    text = text.strip(' ') #Remove leading and trailing spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "wGgh1Un3PcgL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the text into individual word tokens (Tokenization)\n",
        "def tokenText(text):\n",
        "    text = word_tokenize(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "zq75xY7XQ1n2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary of slang words and their standard equivalents\n",
        "slangwords = {\n",
        "    \"@\": \"di\", \"abis\": \"habis\", \"masi\": \"masih\", \"bgt\": \"banget\", \"maks\": \"maksimal\",\n",
        "    \"tp\": \"tapi\", \"jd\": \"jadi\", \"krn\": \"karena\", \"trs\": \"terus\", \"good\": \"bagus\",\n",
        "    \"dmn\": \"dimana\", \"hrs\": \"harus\", \"sy\": \"saya\", \"sm\": \"sama\", \"very good\": \"mantap\",\n",
        "    \"utk\": \"untuk\", \"sbg\": \"sebagai\", \"blm\": \"belum\", \"udh\": \"sudah\", \"sdh\": \"sudah\",\n",
        "    \"skrg\": \"sekarang\", \"dpt\": \"dapat\", \"tdk\": \"tidak\", \"bs\": \"bisa\", \"gk\": \"tidak\",\n",
        "    \"dr\": \"dari\", \"dg\": \"dengan\", \"aja\": \"saja\", \"smua\": \"semua\", \"ngk\": \"tidak\",\n",
        "    \"apk\": \"aplikasi\", \"dev\": \"developer\", \"bug\": \"kesalahan\", \"jlk\": \"jelek\",\n",
        "    \"crash\": \"gagal\", \"sgt\": \"sangat\", \"fitur\": \"fungsi\", \"kcw\": \"kecewa\", \"g\": \"tidak\",\n",
        "    \"err\": \"error\", \"eror\": \"error\", \"lag\": \"lambat\", \"bgs\": \"bagus\", \"gagal load\": \"gagal memuat\",\n",
        "    \"ngebug\": \"bermasalah\", \"lemot\": \"lambat\", \"bagu\": \"bagus\", \"gabisa\": \"tidak bisa\",\n",
        "    \"kehapus\": \"terhapus\", \"ribet\": \"sulit\", \"males\": \"tidak mau\", \"terimakasih\": \"terima kasih\",\n",
        "    \"gampang\": \"mudah\", \"cepet\": \"cepat\", \"error\": \"kesalahan\", \"ngga\": \"tidak\",\n",
        "    \"coba2\": \"mencoba\", \"cape\": \"capek\", \"drpd\": \"daripada\", \"ampun\": \"parah\",\n",
        "    \"parah\": \"buruk\", \"mantep\": \"mantap\", \"makasih\": \"terima kasih\", \"gpp\": \"tidak apa-apa\",\n",
        "    \"bbrp\": \"beberapa\", \"syg\": \"sayang\", \"dmna\": \"dimana\", \"lg\": \"lagi\",\n",
        "    \"stuck\": \"macet\", \"gmn\": \"gimana\", \"kliatan\": \"kelihatan\", \"ampas\": \"buruk\",\n",
        "    \"gajelas\": \"tidak jelas\", \"gaje\": \"tidak jelas\", \"kalo\": \"kalau\", \"td\": \"tadi\",\n",
        "    \"gt\": \"gitu\", \"gitu\": \"begitu\", \"org\": \"orang\", \"blg\": \"bilang\", \"tlg\": \"tolong\",\n",
        "    \"ak\": \"aku\", \"gw\": \"saya\", \"loe\": \"kamu\", \"lo\": \"kamu\", \"gua\": \"saya\",\n",
        "    \"bro\": \"saudara\", \"sis\": \"saudari\", \"kmrn\": \"kemarin\", \"br\": \"baru\",\n",
        "    \"btw\": \"ngomong-ngomong\", \"tq\": \"terima kasih\", \"kpn\": \"kapan\",\n",
        "    \"knp\": \"kenapa\", \"bkn\": \"bukan\", \"aneh\": \"tidak biasa\", \"cmn\": \"cuman\",\n",
        "    \"cm\": \"cuma\", \"kyk\": \"seperti\", \"krna\": \"karena\", \"yg\": \"yang\",\n",
        "    \"lah\": \"\", \"sih\": \"\", \"jgn\": \"jangan\", \"uda\": \"sudah\", \"gws\": \"sehat selalu\",\n",
        "    \"maap\": \"maaf\", \"ajg\": \"anjing\", \"anjir\": \"anjing\", \"mantul\": \"mantap betul\",\n",
        "    \"cmiiw\": \"correct me if I'm wrong\", \"afk\": \"away from keyboard\",\n",
        "    \"plis\": \"tolong\", \"kl\": \"kalau\", \"pdhl\": \"padahal\", \"udah\": \"sudah\",\n",
        "    \"jdwl\": \"jadwal\", \"pk\": \"pakai\", \"prnh\": \"pernah\", \"ky\": \"kayak\",\n",
        "    \"trmksh\": \"terima kasih\", \"sbnrnya\": \"sebenarnya\", \"smpe\": \"sampai\",\n",
        "    \"jdul\": \"judul\", \"cpt\": \"cepat\", \"tlpn\": \"telepon\", \"bsok\": \"besok\",\n",
        "    \"sjk\": \"sejak\", \"gaada\": \"tidak ada\", \"gmna\": \"bagaimana\", \"mo\": \"mau\",\n",
        "    \"bbrapa\": \"beberapa\", \"sdikit\": \"sedikit\", \"lbih\": \"lebih\", \"msh\": \"masih\",\n",
        "    \"ntr\": \"nanti\", \"gtw\": \"gak tau\", \"kek\": \"seperti\", \"mnding\": \"mendingan\",\n",
        "    \"aj\": \"saja\", \"ni\": \"ini\", \"da\": \"sudah\", \"ogut\": \"saya\", \"noob\": \"pemula\",\n",
        "    \"mauu\": \"mau\", \"mls\": \"malas\", \"bngt\": \"banget\", \"mksd\": \"maksud\",\n",
        "    \"trnyata\": \"ternyata\", \"cb\": \"coba\", \"blh\": \"boleh\", \"prcma\": \"percuma\",\n",
        "    \"ktmu\": \"ketemu\", \"dlu\": \"dulu\", \"bener\": \"benar\", \"bner\": \"benar\",\n",
        "    \"kesel\": \"kesal\", \"mendingan\": \"lebih baik\", \"biar\": \"supaya\",\n",
        "    \"akhlak\": \"moral\", \"dah\": \"sudah\", \"skli\": \"sekali\"\n",
        "}"
      ],
      "metadata": {
        "id": "iFkVqesHQ38F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace slang words with their standard equivalents.\n",
        "def replaceSlang(text, slangwords):\n",
        "    words = text.split()\n",
        "    new_words = [slangwords[word] if word in slangwords else word for word in words]\n",
        "    return ' '.join(new_words)"
      ],
      "metadata": {
        "id": "O2ZZvekCQ4ZQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords from the text (Stopword Removal)\n",
        "def filterText(text):\n",
        "    listStopwords = set(stopwords.words(\"indonesian\")) #Load Indonesian stopwords\n",
        "    listStopwords1 = set(stopwords.words(\"english\")) #Load English stopwords\n",
        "    listStopwords.update(listStopwords1) #Combine Indonesian and English stopwords\n",
        "    listStopwords.update([\n",
        "        \"iya\", \"yaa\", \"gak\", \"gk\", \"nya\", \"na\", \"sih\", \"ku\", \"di\", \"ga\", \"ya\", \"gaa\", \"loh\", \"kah\", \"woi\", \"woii\", \"woy\",\n",
        "        \"dong\", \"deh\", \"nih\", \"tuh\", \"klo\", \"mah\", \"lho\", \"kan\", \"kayak\", \"banget\", \"aja\", \"kok\", \"sama\", \"gitu\", \"dah\",\n",
        "        \"lah\", \"tau\", \"udah\", \"belum\", \"emang\", \"eh\", \"masa\", \"kayaknya\", \"soalnya\", \"gimana\", \"kenapa\", \"pokoknya\",\n",
        "        \"apalagi\", \"terus\", \"mending\", \"bakal\", \"tapi\", \"padahal\", \"walaupun\", \"daripada\", \"abis\", \"doang\", \"sangat\",\n",
        "        \"sekali\", \"lebih\", \"paling\", \"tetep\", \"tetapi\", \"sampe\", \"makanya\", \"ke\", \"buat\", \"biar\", \"hampir\",\n",
        "        \"bukan\", \"malah\", \"meskipun\", \"mungkin\", \"so\", \"tp\", \"jd\", \"jg\", \"krn\", \"trs\", \"dmn\", \"hrs\", \"sy\", \"saya\",\n",
        "        \"anda\", \"kamu\", \"kalian\", \"dia\", \"mereka\", \"kita\", \"aku\", \"gua\", \"gw\", \"lu\", \"loe\"\n",
        "    ]) #Add custom stopwords\n",
        "    filtered = [txt for txt in text if txt not in listStopwords]\n",
        "    text = filtered\n",
        "    return text"
      ],
      "metadata": {
        "id": "I3-q4IxCQ6rN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming Process using Sastrawi Library\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "def stemmingText(text, stemmer):\n",
        "    return [stemmer.stem(word) for word in text]"
      ],
      "metadata": {
        "id": "q3SSNBbNQ73t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert a list of words into sentences\n",
        "def toSentence(list_words):\n",
        "    return ' '.join(list_words)"
      ],
      "metadata": {
        "id": "coudMfWxQ85j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Preprocessing New Sample Data**"
      ],
      "metadata": {
        "id": "arJYNDdpRA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting Sentiment on New Data\n",
        "new_texts = [\n",
        "    \"% Aplikasinya keren banget! Mantap.\",\n",
        "    \"Sudah bagus, semangat terus yah devnya\",\n",
        "    \"Saya suka tampilannya, simpel dan mudah digunakan.\",\n",
        "    \"Susah masuk, tapi ya sudah gpp\",\n",
        "    \"Payah banget aplikasinya !!!\",\n",
        "    \"Login cuman disuruh masukin email aja kacauuu.\",\n",
        "    \"Tidak ada masalah sejauh ini, bagus kok!\",\n",
        "]"
      ],
      "metadata": {
        "id": "ThflogGI8E3v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Full preprocessing function\n",
        "df_new = pd.DataFrame({'ulasan': new_texts})\n",
        "\n",
        "df_new['txt_clean'] = df_new['ulasan'].apply(cleanText)\n",
        "df_new['txt_slangwords'] = df_new['txt_clean'].apply(lambda x: replaceSlang(x, slangwords))\n",
        "df_new['txt_tokenText'] = df_new['txt_slangwords'].apply(tokenText)\n",
        "df_new['txt_stopword'] = df_new['txt_tokenText'].apply(filterText)\n",
        "df_new['txt_stemming'] = df_new['txt_stopword'].apply(lambda x: stemmingText(x, stemmer))\n",
        "df_new['fix_text'] = df_new['txt_stemming'].apply(toSentence)\n",
        "\n",
        "clean_texts = df_new['fix_text'].tolist()\n",
        "\n",
        "new_tfidf = tfidf.transform(clean_texts)\n",
        "new_sequences = tokenizer.texts_to_sequences(clean_texts)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=100)"
      ],
      "metadata": {
        "id": "Jz5l5rJvRzuZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Predicting Sentiment from New Data**"
      ],
      "metadata": {
        "id": "sOfQ7-K-SP7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment prediction with various models\n",
        "label_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "#Predict with SVM\n",
        "svm_preds = svm_model.predict(new_tfidf)\n",
        "svm_labels = [label_names[pred] for pred in svm_preds]\n",
        "#Predict with Logistic Regression\n",
        "logreg_preds = logreg_model.predict(new_tfidf)\n",
        "logreg_labels = [label_names[pred] for pred in logreg_preds]\n",
        "#Predict with LSTM\n",
        "lstm_probs = lstm_model.predict(new_padded)\n",
        "lstm_preds = np.argmax(lstm_probs, axis=1)\n",
        "lstm_labels = [label_names[pred] for pred in lstm_preds]\n",
        "for i, text in enumerate(new_texts):\n",
        "    print(f\"Review: {text}\")\n",
        "    print(f\" - SVM Prediction: {svm_labels[i]}\")\n",
        "    print(f\" - Logistic Regression Prediction: {logreg_labels[i]}\")\n",
        "    print(f\" - LSTM Prediction: {lstm_labels[i]}\")\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3fYfLUCSPtx",
        "outputId": "ad154316-d595-44d6-bd99-81c0c41b4c05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851ms/step\n",
            "Review: % Aplikasinya keren banget! Mantap.\n",
            " - SVM Prediction: Positive\n",
            " - Logistic Regression Prediction: Positive\n",
            " - LSTM Prediction: Positive\n",
            "--------------------------------------------------\n",
            "Review: Sudah bagus, semangat terus yah devnya\n",
            " - SVM Prediction: Positive\n",
            " - Logistic Regression Prediction: Positive\n",
            " - LSTM Prediction: Positive\n",
            "--------------------------------------------------\n",
            "Review: Saya suka tampilannya, simpel dan mudah digunakan.\n",
            " - SVM Prediction: Positive\n",
            " - Logistic Regression Prediction: Positive\n",
            " - LSTM Prediction: Positive\n",
            "--------------------------------------------------\n",
            "Review: Susah masuk, tapi ya sudah gpp\n",
            " - SVM Prediction: Neutral\n",
            " - Logistic Regression Prediction: Neutral\n",
            " - LSTM Prediction: Neutral\n",
            "--------------------------------------------------\n",
            "Review: Payah banget aplikasinya !!!\n",
            " - SVM Prediction: Negative\n",
            " - Logistic Regression Prediction: Negative\n",
            " - LSTM Prediction: Negative\n",
            "--------------------------------------------------\n",
            "Review: Login cuman disuruh masukin email aja kacauuu.\n",
            " - SVM Prediction: Negative\n",
            " - Logistic Regression Prediction: Negative\n",
            " - LSTM Prediction: Negative\n",
            "--------------------------------------------------\n",
            "Review: Tidak ada masalah sejauh ini, bagus kok!\n",
            " - SVM Prediction: Positive\n",
            " - Logistic Regression Prediction: Positive\n",
            " - LSTM Prediction: Positive\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}